{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Scrapping was done on script sent to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Part 1\n",
    "Get links to each beer in the Top 100 beer list per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA\n",
    "url_amer_ipa = 'https://www.beeradvocate.com/lists/style/116/'\n",
    "url_ne_ipa = 'https://www.beeradvocate.com/lists/style/189/'\n",
    "url_imp_ipa = 'https://www.beeradvocate.com/lists/style/140/'\n",
    "\n",
    "# html = urlopen(url_amer_ipa)\n",
    "\n",
    "response1 = requests.get(url_amer_ipa)\n",
    "response2 = requests.get(url_ne_ipa)\n",
    "response3 = requests.get(url_imp_ipa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESS\n",
    "page1 = response1.text\n",
    "soup1 = BeautifulSoup(page1, \"lxml\")\n",
    "table1 = soup1.findAll(\"table\")\n",
    "\n",
    "page2 = response2.text\n",
    "soup2 = BeautifulSoup(page2, \"lxml\")\n",
    "table2 = soup2.findAll(\"table\")\n",
    "\n",
    "page3 = response3.text\n",
    "soup3 = BeautifulSoup(page3, \"lxml\")\n",
    "table3 = soup3.findAll(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows1 = table1[0].findAll('tr')[2:]\n",
    "data_rows2 = table2[0].findAll('tr')[2:]\n",
    "data_rows3 = table3[0].findAll('tr')[2:]\n",
    "# data_rows[2] to data_rows[101] contains all of the beers info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive segment of url link for each specific beer in the top 100 list\n",
    "# if I were to create a function for this, I would instatiate a dictionary\\\n",
    "# with each key being name of the list, and the value being the list \n",
    "\n",
    "amer_ipa_list=[]\n",
    "for i in range(len(data_rows1)):\n",
    "    link = data_rows1[i].find('a')['href']\n",
    "    amer_ipa_list.append(link)\n",
    "\n",
    "ne_ipa_list=[]\n",
    "for i in range(len(data_rows2)):\n",
    "    link = data_rows2[i].find('a')['href']\n",
    "    ne_ipa_list.append(link)\n",
    "    \n",
    "imp_ipa_list=[]\n",
    "for i in range(len(data_rows3)):\n",
    "    link = data_rows3[i].find('a')['href']\n",
    "    imp_ipa_list.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to ensure there are no duplicate beers among the 3 diff. top 100 lists\n",
    "l1 = set(amer_ipa_list+ne_ipa_list+imp_ipa_list)\n",
    "len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate first part and second part of url to get to reviews page of each beer\n",
    "amer_ipa_fulllist = ['https://www.beeradvocate.com' + amer_ipa_list[i] +'?view=beer&sort=&start=' for i in range(len(amer_ipa_list))]\n",
    "ne_ipa_fulllist = ['https://www.beeradvocate.com' + ne_ipa_list[i] +'?view=beer&sort=&start=' for i in range(len(ne_ipa_list))]\n",
    "imp_ipa_fulllist = ['https://www.beeradvocate.com' + imp_ipa_list[i] +'?view=beer&sort=&start=' for i in range(len(imp_ipa_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.beeradvocate.com/beer/profile/13014/138735/?view=beer&sort=&start='"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "amer_ipa_fulllist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Part 2\n",
    "Get reviews for each beer in the Top 100 beer list per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = amer_ipa_fulllist+ne_ipa_fulllist+imp_ipa_fulllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('beer_list.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_list, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('beer_list.pkl', 'rb') as f:\n",
    "    f_list = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.beeradvocate.com/beer/profile/13014/138735/?view=beer&sort=&start=',\n",
       " 'https://www.beeradvocate.com/beer/profile/22511/160794/?view=beer&sort=&start=',\n",
       " 'https://www.beeradvocate.com/beer/profile/44221/247726/?view=beer&sort=&start=',\n",
       " 'https://www.beeradvocate.com/beer/profile/3120/32286/?view=beer&sort=&start=',\n",
       " 'https://www.beeradvocate.com/beer/profile/30654/262996/?view=beer&sort=&start=']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "print(len(f_list))\n",
    "f_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "combined_list[0:5]\n",
    "len(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_reviews =[]\n",
    "# list_of_reviews2 =[]\n",
    "# list_of_reviews3 =[]\n",
    "list_of_reviews =[]\n",
    "\n",
    "\n",
    "#loop through each link \n",
    "# for i in range(len(combined_list)):  # change back to this to scrap all 300 beers\n",
    "# for i in range(100,175):  # Second batch, list_of_reviews2\n",
    "# for i in range(175,200):  # Second batch, list_of_reviews3\n",
    "for i in range(200,300):  # Second batch, list_of_reviews4\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    start = 0\n",
    "    page_end = 2 #integer representing the last page, 2 is placeholder  \n",
    "    count = 0\n",
    "    \n",
    "    while count <= page_end:\n",
    "        if count ==0:\n",
    "            # first access the the inital webpage (start=0)\n",
    "            new_url = combined_list[i]+'0'\n",
    "            new_response = requests.get(new_url)\n",
    "            new_page = new_response.text\n",
    "            new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "            \n",
    "            # find number of ratings to determine number of iterations\n",
    "            rating = new_soup.findAll(class_='ba-ratings')\n",
    "            rc = rating[0].text\n",
    "            rating_count = int(rc.replace(',',''))\n",
    "            # divide rating by 25 and round down to get number of iterations\n",
    "            ct = rating_count//25 \n",
    "            # return last page (intger)\n",
    "            page_end = ct*25\n",
    "            \n",
    "            # retrieve reviews on page 1\n",
    "            review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "            \n",
    "            #Add check for # of reviews on page\n",
    "            for reviewer in range(len(review)):\n",
    "                rev_comment = list(review[reviewer])\n",
    "                # check to see if there actually is a review\n",
    "                if len(rev_comment)>7:\n",
    "                    list_of_reviews.append(rev_comment)\n",
    "            count +=25\n",
    "            \n",
    "        else:\n",
    "            # retrieve reviews for rest of the pages\n",
    "            new_url = combined_list[i]+str(count)\n",
    "            # add delay to prevent time out\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(5)\n",
    "            if count % 1000 == 0:\n",
    "                time.sleep(2.5)\n",
    "            if count % 100 == 0:\n",
    "                time.sleep(2)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    new_response = requests.get(new_url)\n",
    "                except:\n",
    "                    time.sleep(60)\n",
    "                    try:\n",
    "                        new_response = requests.get(new_url)\n",
    "                    except:\n",
    "                        time.sleep(120)\n",
    "                        pass #(continue or pass?)\n",
    "                new_page = new_response.text\n",
    "                new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "                review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "\n",
    "                #Add check for # of reviews on page\n",
    "                for reviewer in range(len(review)):\n",
    "                    rev_comment = list(review[reviewer])\n",
    "                    # check to see if there actually is a review\n",
    "                    if len(rev_comment)>7:\n",
    "                        # copy from below paste here\n",
    "                        list_of_reviews.append(rev_comment)\n",
    "            count +=25    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index where connection timed out, 99th link\n",
    "amer_ipa_fulllist.index('https://www.beeradvocate.com/beer/profile/287/1093/?view=beer&sort=&start=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61065"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link 0 to 99 \n",
    "list1 = list_of_reviews\n",
    "len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64818"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link 100 to 175 \n",
    "list2 = list_of_reviews2\n",
    "len(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link 175 to 200 \n",
    "list3 = list_of_reviews3\n",
    "len(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57026"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link 200 to 300 \n",
    "list4 = list_of_reviews4\n",
    "len(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_rev_list = list1 + list2 + list3 +list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188305"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_rev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Part 3 \n",
    "Extract actual reviews from list of html elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract actual reviews out of list_of_reviews\n",
    "letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "str_list_reviews = []\n",
    "\n",
    "for list_ in combined_rev_list:\n",
    "    for i in range(len(list_)):\n",
    "        if str(list_[i])[0] in letters:\n",
    "            str_list_reviews.append(list_[i])\n",
    "            \n",
    "\n",
    "# Implement this into the code above within the loop. \n",
    "# Write to csv as I go so I don't lose any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['12 ounce bottle poured into a snifter.',\n",
       " 'on tap at sams in Durham into a bells pint glass.',\n",
       " \"Bell's \",\n",
       " 'Served on tap into tulip glass.',\n",
       " 'Bottled on 1/23/13 very fresh',\n",
       " \"Appearance - Beautiful crystal clear copper. I poured it pretty conservatively so there wasn't a great deal of head. Nice lacing.\",\n",
       " 'Big hoppy and malty. Heavy beer. Not very hop forward in the aroma. Sweet. Enjoyable still but not even close to the drinking enjoyment level of hundreds of IPAs being made these days.',\n",
       " '12oz. bottle poured into a room-temp pint glass. The bottling date is 1/23/13.',\n",
       " 'Had never heard of this beer until now. As I understand it is a limited brew, I will continue to order until it runs out. I have to be careful as the 10% ABV knocks me in the dirt pretty quick. With the aroma and taste, it goes down smooth with a pleasant linger on the pallate, which makes it very easy to have too many too quick. There is a nice hint of citrus. Just right.',\n",
       " 'Hopslam FINALLY became available at my local beer emporium, so I forked over $14 (!) for a six pack. Did reality meet the hype?']"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "print(len(str_list_reviews))\n",
    "str_list_reviews[47000:47010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514568"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(str_list_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import sys\n",
    "# sys.setrecursionlimit(60000)\n",
    "\n",
    "# with open('beer_rev_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(str_list_reviews, f)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(str_list_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testlist=[]\n",
    "# # retrieve reviews on page 1\n",
    "# new_url = combined_list[0]+'100'\n",
    "# new_response = requests.get(new_url)\n",
    "# new_page = new_response.text\n",
    "# new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "\n",
    "# review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "\n",
    "# for reviewer in range(1):\n",
    "#     rev_comment = list(review[reviewer])\n",
    "#     # check to see if there actually is a review\n",
    "#     if len(rev_comment)>=9 and str(rev_comment[8])[0] in letters:\n",
    "#             testlist.append(rev_comment[8])\n",
    "        \n",
    "# len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this returns the reviewer's rating for each category: look,smell,taste, feel, overall\n",
    "# # however, this is returning different beers\n",
    "# rating_table = new_soup.findAll(class_=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. Tokenization with nltk.tokenize\n",
    "    * Sentence tokenization\n",
    "    * word tokenization\n",
    "    * pre-tokenization:\n",
    "        * lowercase?\n",
    "        * stop words?  \n",
    "        \n",
    "2. Parts of Speech Tagging\n",
    "    * from nltk.tag import pos_tag  \n",
    "    \n",
    "3. Chunking (extracting phrases) ?\n",
    "    *Install these?\n",
    "        * movie_reviews: Imdb reviews characterized as pos & neg\n",
    "        * treebank: tagged and parsed Wall Street Journal text\n",
    "        * brown: tagged & categorized English text (news, fiction, etc)\n",
    "        * from nltk.corpus import treebank_chunk  \n",
    "        \n",
    "4. TextBlob Sentiment Analysis\n",
    "    * gatsby\n",
    "    * Stemming (optional, depends)\n",
    "    * Lemmintization (optional, depends)\n",
    "    * from textblob import TextBlob\n",
    "    * from nltk.corpus import movie_reviews    \n",
    "    \n",
    "5. Bigrams, Trigrams, N-grams (optional?)\n",
    "    * from nltk.util import ngrams\n",
    "        from collections import Counter\n",
    "        from operator import itemgetter\n",
    "        from nltk.corpus import stopwords    \n",
    "        \n",
    "6. CountVectorizer: Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation. (This step comes after you completed all preprocessing)\n",
    "    * from sklearn.feature_extraction.text import CountVectorizer\n",
    "    * from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "    * TF: frequency in this document\n",
    "    * IDF: inverse frequency in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RERUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('beer_list.pkl', 'rb') as f:\n",
    "    combined_list = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('beerreviews1.csv', 'w') as csvfile:\n",
    "    csvfile.write('')\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_reviews=[]\n",
    "\n",
    "#loop through each link \n",
    "for i in range(len(combined_list)):  # change back to this to scrap all 300 beers\n",
    "# for i in range(1):  \n",
    "    letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    start = 0\n",
    "    page_end = 2 #integer representing the last page, 2 is placeholder  \n",
    "    count = 0\n",
    "    \n",
    "    while count <= page_end:\n",
    "        if count ==0:\n",
    "            # first access the the inital webpage (start=0)\n",
    "            new_url = combined_list[i]+'0'\n",
    "            new_response = requests.get(new_url)\n",
    "            new_page = new_response.text\n",
    "            new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "            \n",
    "            # find number of ratings to determine number of iterations\n",
    "            rating = new_soup.findAll(class_='ba-ratings')\n",
    "            rc = rating[0].text\n",
    "            rating_count = int(rc.replace(',',''))\n",
    "            # divide rating by 25 and round down to get number of iterations\n",
    "            ct = rating_count//25 \n",
    "            # return last page (intger)\n",
    "            page_end = ct*25\n",
    "            \n",
    "            # retrieve reviews on page 1\n",
    "            review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "            \n",
    "            #Add check for # of reviews on page\n",
    "            for reviewer in range(len(review)):\n",
    "                rev_comment = list(review[reviewer])\n",
    "                # check to see if there actually is a review\n",
    "                if len(rev_comment)>7:\n",
    "                    for i in range(len(rev_comment)):\n",
    "                        if str(rev_comment[i])[0] in letters:\n",
    "                            with open('beerreviews1.csv', 'a') as csvfile:\n",
    "                                writer=csv.writer(csvfile)\n",
    "                                writer.writerow([rev_comment[i]])\n",
    "            count +=25\n",
    "            \n",
    "        else:\n",
    "            # retrieve reviews for rest of the pages\n",
    "            new_url = combined_list[i]+str(count)\n",
    "            # add delay to prevent time out\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(2)\n",
    "            if count % 1000 == 0:\n",
    "                time.sleep(5)\n",
    "#             if count % 100 == 0:\n",
    "#                 time.sleep(1)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    new_response = requests.get(new_url)\n",
    "                except:\n",
    "                    time.sleep(60)\n",
    "                    try:\n",
    "                        new_response = requests.get(new_url)\n",
    "                    except:\n",
    "                        time.sleep(120)\n",
    "                        pass #(continue or pass?)\n",
    "                new_page = new_response.text\n",
    "                new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "                review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "\n",
    "                #Add check for # of reviews on page\n",
    "                for reviewer in range(len(review)):\n",
    "                    rev_comment = list(review[reviewer])\n",
    "                    # check to see if there actually is a review\n",
    "                    if len(rev_comment)>7:\n",
    "                        for i in range(len(rev_comment)):\n",
    "                            if str(rev_comment[i])[0] in letters:\n",
    "                                with open('beerreviews1.csv', 'a') as csvfile:\n",
    "                                    writer=csv.writer(csvfile)\n",
    "                                    writer.writerow([rev_comment[i]])                                \n",
    "            count +=25    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.beeradvocate.com/beer/profile/38140/238158/?view=beer&sort=&start=0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_url = combined_list[i]+'0'\n",
    "new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citra forward (pineapple, grapefruit, tangerine etc), mild bitter finish, it is good but doesn’t really stand out from the crowd though that said it is still quietly very good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What?!?! This beer is hop magic!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The stamp is a bit smudged, but it looks like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow isn't that the most meanest &amp; toughest bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aggressive but good. Citrus aroma, nice color ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There is a reason Beer Connoisseur rated this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Citra forward (pineapple, grapefruit, tangerine etc), mild bitter finish, it is good but doesn’t really stand out from the crowd though that said it is still quietly very good\n",
       "0                   What?!?! This beer is hop magic!                                                                                                                             \n",
       "1  The stamp is a bit smudged, but it looks like ...                                                                                                                             \n",
       "2  Wow isn't that the most meanest & toughest bee...                                                                                                                             \n",
       "3  Aggressive but good. Citrus aroma, nice color ...                                                                                                                             \n",
       "4  There is a reason Beer Connoisseur rated this ...                                                                                                                             "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('beerreviews1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9679, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify & Rerun orginal Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('beerreviews2.1.csv', 'w') as csvfile:\n",
    "    csvfile.write('')\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_reviews=[]\n",
    "\n",
    "#loop through each link \n",
    "for i in range(len(combined_list[0:100])):  # change back to this to scrap all 300 beers\n",
    "# for i in range(1):  \n",
    "    letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    start = 0\n",
    "    page_end = 2 #integer representing the last page, 2 is placeholder  \n",
    "    count = 0\n",
    "    \n",
    "    while count <= page_end:\n",
    "        if count ==0:\n",
    "            # first access the the inital webpage (start=0)\n",
    "            new_url = combined_list[i]+'0'\n",
    "            new_response = requests.get(new_url)\n",
    "            new_page = new_response.text\n",
    "            new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "            \n",
    "            # find number of ratings to determine number of iterations\n",
    "            rating = new_soup.findAll(class_='ba-ratings')\n",
    "            rc = rating[0].text\n",
    "            rating_count = int(rc.replace(',',''))\n",
    "            # divide rating by 25 and round down to get number of iterations\n",
    "            ct = rating_count//25 \n",
    "            # return last page (intger)\n",
    "            page_end = ct*25\n",
    "            \n",
    "            # retrieve reviews on page 1\n",
    "            review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "            \n",
    "            #Add check for # of reviews on page\n",
    "            for reviewer in range(len(review)):\n",
    "                rev_comment = list(review[reviewer])\n",
    "                # check to see if there actually is a review\n",
    "                if len(rev_comment)>7:\n",
    "                    for i in range(len(rev_comment)):\n",
    "                        if str(rev_comment[i])[0] in letters:\n",
    "                            with open('beerreviews2.1.csv', 'a') as csvfile:\n",
    "                                writer=csv.writer(csvfile)\n",
    "                                writer.writerow([rev_comment[i]])\n",
    "            count +=25\n",
    "            \n",
    "        else:\n",
    "            # retrieve reviews for rest of the pages\n",
    "            new_url = combined_list[i]+str(count)\n",
    "            # add delay to prevent time out\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(2)\n",
    "            if count % 1000 == 0:\n",
    "                time.sleep(3)\n",
    "#             if count % 100 == 0:\n",
    "#                 time.sleep(1)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    new_response = requests.get(new_url)\n",
    "                except:\n",
    "                    time.sleep(30)\n",
    "                    try:\n",
    "                        new_response = requests.get(new_url)\n",
    "                    except:\n",
    "                        time.sleep(60)\n",
    "                        pass \n",
    "                new_page = new_response.text\n",
    "                new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "                review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "\n",
    "                #Add check for # of reviews on page\n",
    "                for reviewer in range(len(review)):\n",
    "                    rev_comment = list(review[reviewer])\n",
    "                    # check to see if there actually is a review\n",
    "                    if len(rev_comment)>7:\n",
    "                        for i in range(len(rev_comment)):\n",
    "                            if str(rev_comment[i])[0] in letters:\n",
    "                                with open('beerreviews2.1.csv', 'a') as csvfile:\n",
    "                                    writer=csv.writer(csvfile)\n",
    "                                    writer.writerow([rev_comment[i]])                                \n",
    "            count +=25    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3124"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('beerreviews2.1.csv')\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('beerreviews3.1.csv', 'w') as csvfile:\n",
    "    csvfile.write('')\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_reviews =[]\n",
    "# list_of_reviews2 =[]\n",
    "# list_of_reviews3 =[]\n",
    "# list_of_reviews =[]\n",
    "\n",
    "\n",
    "#loop through each link \n",
    "for i in range(0,100):  # Second batch, list_of_reviews4\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    start = 0\n",
    "    page_end = 2 #integer representing the last page, 2 is placeholder  \n",
    "    count = 0\n",
    "    \n",
    "    while count <= page_end:\n",
    "        if count ==0:\n",
    "            # first access the the inital webpage (start=0)\n",
    "            new_url = combined_list[i]+'0'\n",
    "            new_response = requests.get(new_url)\n",
    "            new_page = new_response.text\n",
    "            new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "            \n",
    "            # find number of ratings to determine number of iterations\n",
    "            rating = new_soup.findAll(class_='ba-ratings')\n",
    "            rc = rating[0].text\n",
    "            rating_count = int(rc.replace(',',''))\n",
    "            # divide rating by 25 and round down to get number of iterations\n",
    "            ct = rating_count//25 \n",
    "            # return last page (intger)\n",
    "            page_end = ct*25\n",
    "            \n",
    "            # retrieve reviews on page 1\n",
    "            review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "            \n",
    "            #Add check for # of reviews on page\n",
    "            for reviewer in range(len(review)):\n",
    "                rev_comment = list(review[reviewer])\n",
    "                # check to see if there actually is a review\n",
    "                if len(rev_comment)>7:\n",
    "                    for i in range(len(rev_comment)):\n",
    "                        if str(rev_comment[i])[0] in letters:\n",
    "                            with open('beerreviews3.1.csv', 'a') as csvfile:\n",
    "                                writer=csv.writer(csvfile)\n",
    "                                writer.writerow([rev_comment[i]])\n",
    "            count +=25\n",
    "            \n",
    "        else:\n",
    "            # retrieve reviews for rest of the pages\n",
    "            new_url = combined_list[i]+str(count)\n",
    "            # add delay to prevent time out\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(1)\n",
    "            if count % 2000 == 0:\n",
    "                time.sleep(2)\n",
    "#             if count % 100 == 0:\n",
    "#                 time.sleep(2)\n",
    "\n",
    "#             else:\n",
    "#                 try:\n",
    "                new_response = requests.get(new_url)\n",
    "#                 except:\n",
    "#                     time.sleep(60)\n",
    "#                     try:\n",
    "#                         new_response = requests.get(new_url)\n",
    "#                     except:\n",
    "#                         time.sleep(120)\n",
    "#                         pass #(continue or pass?)\n",
    "                new_page = new_response.text\n",
    "                new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "                review = new_soup.findAll(id='rating_fullview_content_2')\n",
    "\n",
    "                #Add check for # of reviews on page\n",
    "                for reviewer in range(len(review)):\n",
    "                    rev_comment = list(review[reviewer])\n",
    "                    # check to see if there actually is a review\n",
    "                    if len(rev_comment)>7:\n",
    "                        for i in range(len(rev_comment)):\n",
    "                            if str(rev_comment[i])[0] in letters:\n",
    "                                with open('beerreviews3.1.csv', 'a') as csvfile:\n",
    "                                    writer=csv.writer(csvfile)\n",
    "                                    writer.writerow([rev_comment[i]]) \n",
    "            count +=25    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('beerreviews3.1.csv')\n",
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3129"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv('beerreviewsv2.csv',header=None)\n",
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()\n",
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Tap at Mike Taproom Largo,FL Pours a golden color with a fresh small white head. Smell was floral,citrus and dank hop. Taste followed nose was crisp with hops and citrus forward . Mouth feel crisp with some pine bitterness in finish but floral/tropical hops are in charge here. A delightful IPA.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.loc[2592][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15840"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.read_csv('beerreviewsv2.1.csv',header=None)\n",
    "len(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It’s spiritual!!! Listen to your favorite music and prepare to enjoy!!!!!'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.loc[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
